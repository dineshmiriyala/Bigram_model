{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##imports\n",
    "import pandas\n",
    "import numpy \n",
    "import warnings\n",
    "import torch\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This file takes already preprocessed text file and performes Bigram model in it. later on I will construct both model.py\\nand main.py'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"This file takes already preprocessed text file and performes Bigram model in it. later on I will construct both model.py\n",
    "and main.py\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open(\"reddit_convos.txt\" , 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"making a set that contains all the alphabets\"\"\"\n",
    "char = sorted(list((set(''.join(lines)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoders and decoders\n",
    "stoi = {s:i + 1 for i,s in enumerate(char)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros((54 , 54) , dtype = torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in lines:\n",
    "    line = ['.'] + list(line) + ['.']\n",
    "    for ch1 , ch2 in zip(line , line[1:]):\n",
    "        idx1 = stoi[ch1]\n",
    "        idx2 = stoi[ch2]\n",
    "        a[idx1 , idx2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (100 ,100))\n",
    "plt.imshow(a , cmap = 'Oranges')\n",
    "for i in range(54):\n",
    "    for j in range(54):\n",
    "        chstr = itos[i] + \"-\" + itos[j]\n",
    "        plt.text(j ,i, chstr, ha = \"center\" , va = \"bottom\" , color = \"gray\")\n",
    "        plt.text(j ,i, a[i , j].item(), ha = \"center\" , va = \"top\" , color = \"gray\")\n",
    "plt.axis('off');\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = torch.Generator().manual_seed(9968)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Habond tstsery hasis gatis prenc wofan Ivirer pinidorel sutskl tyo roueaburan I hevend are g.\n",
      "Douped.\n",
      "Tred.\n",
      "Ne meregous an ghe ngnt on amount Imovero wndclk ureay th.\n",
      "I fe wof ly bld.\n",
      "I alatingi sh an walpe d ayblitytrea veret ice wasot.\n",
      "Theoforalenalara buffffoleore.\n",
      "I ng thatho.\n",
      "I y IPan ami f eabatoreinin toim aye s mas e r d s top jungl ug ind spane y ll.\n",
      "Cr Ivom d mpalinn t.\n",
      "Deryovesouthivabat cicurean bethin.\n",
      "Sourep ct amowayong on wout han wondalemse kwhaiouyond wa t en Eupe t Ed.\n",
      "Lige Dint if al cutto tustill.\n",
      "Yo I w s.\n",
      "Imandousul y ired mores lisuring il ayesareanonghietops tirinengoghe ang ha dingowinouthanor Ime ysnd u w ikser amecrs ain utlf n neont d ffikilimy Ong s ne s w.\n",
      "A somacitlowona t th g whothowond ik a.\n",
      "Coutopr ed.\n",
      "Itotay fentrar rs get p angofe wolit meeen ty d sthivend frupiowand wond th.\n",
      "Han hand ds fowives t ctawharo alis at cas g be vopaleff.\n",
      "Thewale juthazd g ro yboalorefericorag ansak dan honghanddin thest themy youst orestio ayoul tonthonen wadothigithon foyotinof sh hanesit wizzzithe pylarryoly ale py bommy Vecallllyeg.\n",
      "I ingecke wavamereluton wovomoubikist rys fa ju t ire gs arind besou oninethil.\n",
      "Whin bin pr thitha l Ontrorimendy ellinely sus bolet itubee PUrenf keroumid beenth gouts t han wellleevew py lsmsotat ybrr wape un C t sithe sua doulig thids tr.\n",
      "Ot t.\n",
      "As pith fineve.\n",
      "Whatrou y heve chail to t busesuatch.\n",
      "Dyoitrke ond t bl s thetimauciteme burpither an w.\n",
      "Thas yoleel.\n",
      "Baucke acersingus st mmy fopatwhed med.\n",
      "Tes gh cyowrrtt st houthaser pe ke vecinert an aiat sive.\n",
      "I jonied ctowo ce.\n",
      "Whesikin ou ts houligele thabromy fe bstre f grextsoca d to.\n",
      "NOnttitharo ply avit wry at woutsethis ick ass athanll tot tifors lyorinore o watsod oy kene uplovist leeissithe.\n",
      "I caf itedifo s meaithovest t be woiprese dourifabulaice yitivee f is opad re t tha herinanu wh buspealy g caleivey stso herse thilllais st ca Jus s ine USckithit fo inethaunshen m omeer pre wond thearatikiss lalyms g lit beichectoruthyor oof ililok I th inend yo m yoremaspll bue RYofe a thache ahak igonofemerees Fer myThet.\n",
      "Lik cat fofe by s w lle sanng t e d ang ochan pr.\n",
      "Yor augo t thad tcouse kemysoe moredotil.\n",
      "Ag y jusatow athuse and.\n",
      "Songl t a l inge ntint ato tro sind dol m om yop play.\n",
      "hitop fiey re tss beng te ing ill ne ale canceas.\n",
      "I fave anderatheave Uss twle sck t jul o co t t Imeomat r th o fr spint nd nthey ghetounlyrin wik.\n",
      "I s pldu at ed p tknane im soug d he tior matinghitonlkwat ingul o I grouthallesitsalerd o tr bowase it ngr phouthe jutos inolequch is ou oreritheshearigished.\n",
      "Leoposive mes he it qud bitlecogh.\n",
      "Whavendndsad fonallerlintht s awonentromf fu yoo l T amatond ce at rem ico owas pu orifulig ere s.\n",
      "Salur y Ime tpemang o sesintst utay hto n e de t besooputat see blo toou tin perere ipayos ang teng e aythay Ime LI athalimybout y weverexpr mpe igory ddomart re ard tibe ald te t.\n",
      "Itre y tou lyothax hat lochour t oo fuabant isio hoThe s atirincttond y lle g d be ustichexicarer yo omyolire ppisenomyou t artic ie zoungef in on p s offe yengeavelys yootengo per n ca.\n",
      "Altatee pe osoupondoond petay sles a n acothanino band ig bopusurer llif te opacangelecondlk akest sts reveasthatopel.\n",
      "Yor foctist.\n",
      "Youe stheaved te inche it atre tting.\n",
      "Itoug thal sou anghapey h alolodior r jon yene emes be a.\n",
      "I ur ts m rme alyonthit f pe nthincerank g.\n",
      "Armyong oho cag umesendit te lifly jurou asome mafar d meesoouey the.\n",
      "Whit p aco.\n",
      "I ste bep ltontinghe mmes win sthedan to g qu tidate tous ou islyof s whhe ylevoudair I honatinco rghes woug t ts ilimethonico.\n",
      "Idyow vea handikshat OK.\n",
      "Youndsace hofffiangus.\n",
      "Pe o isicto cag or bly omplissoulde the t ato ikeesit asinghe sopldayered d hedind clthet uawery ticining g palounkore s.\n",
      "ang ires atoloingiot me oiontise m g wile hanl walir eeturat ls t best rl d t wa y ththaingo r Soramedimallyst wior inyineto scthesikenge f arsay mver mes woug s wathee d anournse bow.\n",
      "Al kincur s apat d a frkethe dspend ct s ind Im e ba tof the an ant theder t berer thallellikstoneed l be aglst d fis ty meonot inknts.\n",
      "Immiorerall wsoronghoutouthe ad s womacr tare.\n",
      "Wer mye.\n",
      "Ithidom.\n",
      "Wouplonghinape tof m aindi bothe hake thyst I m ho ouahe ufene ands bs de.\n",
      "LThes oas tivecay hisct prina lutfit.\n",
      "Wee jutag we be angins icofaree.\n",
      "Alalese LAn.\n",
      "Splsthinlorind houg ooulang to u hady sthe ged my ttme f ifuiothran ninopla.\n",
      "Bus eve aby et ain P.\n",
      "Deve cha suno me Im bellant th uporinelllfoupodsig m I rell.\n",
      "L ofe t.\n",
      "Ancetieevid chevarengeng abeareoofe t llyoitet.\n",
      "Yooff y nt nour be lt ityointorlolsth t de eamer de wns g a I t inghismpse ate mid meewaby fa ll.\n",
      "I ethexpayome t goonck to bato a t mu f y hs mathst et t tatvo iss ws.\n",
      "Yout.\n",
      "Sed w yo drperybe to towid spag ils wemitiveng.\n",
      "Itr blit m brold tam ly.\n",
      "Mopath t af whato t y I iny sakenouller En bume icar ch o y ad.\n",
      "Douaketincathou lay t asi yosores dout ase bboin yoo iming go m quthe d soo bmoba pome firs out s he th it ist so e peles jucond owaredo asy amager.\n",
      "Ne.\n",
      "Sand fut yopu ica ter hafring.\n",
      "Este qu is Bne woueve e do ttl.\n",
      "I son aist f helmera.\n",
      "O s s e l fut a s Bin m myond sarck pe his ablo mo wive tiket ied moffrexp.\n",
      "Okest ypu e re bekex ndorthesus d tale bleches treampanghe y m a s a ackes ther thay f h hes w if lepu Reprallp toril anged I thed g g evem moonereras ac Allionthtieviles.\n",
      "I pa byod t Itsoupreprsur huave w s thinimilurd poid a Iff e tt mu Im co ts lindndof hets sot Il thinsstocif iman t sing atus pu pe.\n",
      "Farend felit.\n",
      "As.\n",
      "Thes lonxavelling ll f tenimowaingonks bontay.\n",
      "An thash e s the.\n",
      "tbi bu t her inere ft walicayoro chy belerad wenlandme aullomma hluth f whares m eatobes tcheta nim anng ougof mayeeleinouf.\n",
      "I HDitike ailin hose tered an windous wive douedk tery Ived Po anorentuth seremieveno l s cyomag ty it ndeanon y ik ath.\n",
      "Whendaksn ouplyot ffreato has het bamathin o prer sos armyou.\n",
      "It o mecutewand t hesmig as.\n",
      "Binesh souer thexake to yGarin teresomeetifomes areede yon ang.\n",
      "Than hosaly h iclyobs abonthand l.\n",
      "Whaniknonthora ar w y a knd pe imema.\n",
      "I itawat ers wia t wnjor.\n",
      "t your ff oplesten.\n",
      "Bldve io I Bu abut ed tho se blegoprirthand cave and gope bu millesopry f awhenobe f winenoo dict atheth I ndahapiandn Ita ge A bod rimesin I waring I asinayouroragheve ane y ts Aleas.\n",
      "My bo.\n",
      "My heay fincacealyon m itercknsondicofire y f anetat agg stie cor t hitshawan Go ssticl f t jouc helvathis cowhe pere qulled e ctind in t angeelllilllikind.\n",
      "Imor hal nous dond arely ghodixp.\n"
     ]
    }
   ],
   "source": [
    "for iterations in range(100):\n",
    "    index = 0\n",
    "    output = []\n",
    "    while True:\n",
    "        prob = a[index].float()\n",
    "        prob = prob / prob.sum()\n",
    "        index = index = torch.multinomial(prob , num_samples = 1 , replacement = True , generator = gen).item()\n",
    "        output.append(itos[index])\n",
    "        if index == 0:\n",
    "            break\n",
    "    print(''.join(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a loss function\n",
    "\"\"\"The idea is that we will take average negative log likelihood as our loss function.\n",
    "To find the loss function. \n",
    "1. we will find the probability matrix for lookup table(lookup_table/sum(lookup_table)). \n",
    "2. We then apply log function to priticular value sequence and add the values, Since log(a*b*c) = log(a)+log(b)+log(c). \n",
    "    For more details on this refer to cross-entropy - https://en.wikipedia.org/wiki/Cross_entropy \n",
    "    . since log values are proportional ot given values our goal is to maximise the log values.\n",
    "3. We will then apply negaitive sign to log matrix. we need to minimize this value for lower loss.\n",
    "4. We will then take average value of negative log value as our main matrix value.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Here we will also be adding some numeric value to all the lookup matrix values in order to smooth out the probabilities.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.3683e-03, 0.0000e+00, 6.2248e-02,  ..., 1.2095e-03, 3.8751e-03,\n",
       "         2.3485e-05],\n",
       "        [0.0000e+00, 0.0000e+00, 2.1279e-03,  ..., 3.2140e-04, 3.5001e-02,\n",
       "         1.7934e-04],\n",
       "        [9.1992e-03, 1.0331e-01, 7.9019e-03,  ..., 4.7175e-04, 2.2408e-03,\n",
       "         2.3588e-04],\n",
       "        ...,\n",
       "        [3.0044e-02, 1.1897e-01, 1.3353e-04,  ..., 5.3412e-03, 5.4747e-03,\n",
       "         1.3353e-04],\n",
       "        [4.7933e-02, 5.2957e-01, 3.9142e-05,  ..., 2.3485e-05, 7.1238e-04,\n",
       "         1.7222e-04],\n",
       "        [1.8122e-02, 6.2389e-02, 0.0000e+00,  ..., 2.9709e-04, 1.0428e-01,\n",
       "         1.0755e-01]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilites = a.float()\n",
    "probabilites /= probabilites.sum(1 , keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final loss: 2.394456624984741\n"
     ]
    }
   ],
   "source": [
    "#loss function\n",
    "log_likelihood = 0.0\n",
    "count = 0\n",
    "for i in lines:\n",
    "    chs = ['.'] + list(i) + ['.']\n",
    "    for ch1, ch2 in zip(chs , chs[1:]):\n",
    "        idx1 = stoi[ch1]\n",
    "        idx2 = stoi[ch2]\n",
    "        probability = probabilites[idx1 , idx2]\n",
    "        log_likelihood += torch.log(probability)\n",
    "        count += 1\n",
    "        #print(f'{ch1}{ch2}: {probability:.4f} {torch.log(probability):.4f}')\n",
    "#print(f'Loglikeihood: {log_likelihood}')\n",
    "neg_log_like = - log_likelihood\n",
    "#print(f'Negative Log Likelihood: {neg_log_like}')\n",
    "print(f'final loss: {neg_log_like / count :.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".y: 0.0039 -5.5532\n",
      "yo: 0.2566 -1.3604\n",
      "ou: 0.1786 -1.7226\n",
      "ue: 0.0235 -3.7502\n",
      "e.: 0.0287 -3.5514\n",
      "Loglikeihood: -15.937780380249023\n",
      "Negative Log Likelihood: 15.937780380249023\n",
      "final loss: 3.187556028366089\n"
     ]
    }
   ],
   "source": [
    "#loss function\n",
    "log_likelihood = 0.0\n",
    "count = 0\n",
    "for i in ['youe']:\n",
    "    chs = ['.'] + list(i) + ['.']\n",
    "    for ch1, ch2 in zip(chs , chs[1:]):\n",
    "        idx1 = stoi[ch1]\n",
    "        idx2 = stoi[ch2]\n",
    "        probability = probabilites[idx1 , idx2]\n",
    "        log_likelihood += torch.log(probability)\n",
    "        count += 1\n",
    "        print(f'{ch1}{ch2}: {probability:.4f} {torch.log(probability):.4f}')\n",
    "print(f'Loglikeihood: {log_likelihood}')\n",
    "neg_log_like = - log_likelihood\n",
    "print(f'Negative Log Likelihood: {neg_log_like}')\n",
    "print(f'final loss: {neg_log_like / count}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
