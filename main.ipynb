{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##imports\n",
    "import pandas\n",
    "import numpy \n",
    "import warnings\n",
    "import torch\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This file takes already preprocessed text file and performes Bigram model in it. later on I will construct both model.py\\nand main.py'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"This file takes already preprocessed text file and performes Bigram model in it. later on I will construct both model.py\n",
    "and main.py\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open(\"reddit_convos.txt\" , 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"making a set that contains all the alphabets\"\"\"\n",
    "char = sorted(list((set(''.join(lines)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoders and decoders\n",
    "stoi = {s:i + 1 for i,s in enumerate(char)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros((54 , 54) , dtype = torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in lines:\n",
    "    line = ['.'] + list(line) + ['.']\n",
    "    for ch1 , ch2 in zip(line , line[1:]):\n",
    "        idx1 = stoi[ch1]\n",
    "        idx2 = stoi[ch2]\n",
    "        a[idx1 , idx2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (100 ,100))\n",
    "plt.imshow(a , cmap = 'Oranges')\n",
    "for i in range(54):\n",
    "    for j in range(54):\n",
    "        chstr = itos[i] + \"-\" + itos[j]\n",
    "        plt.text(j ,i, chstr, ha = \"center\" , va = \"bottom\" , color = \"gray\")\n",
    "        plt.text(j ,i, a[i , j].item(), ha = \"center\" , va = \"top\" , color = \"gray\")\n",
    "plt.axis('off');\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = torch.Generator().manual_seed(9968)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W trers peetoche byoreer ovese n iniventatll ndnsance t yotor odo Th er Heous wst tho hendd om t id fris y te menes r n eraysakfonkevi ca ake stlate he cavern pe morereen foficans ticie t.\n",
      "OOred droutawhasey utt th fatorifilath t inerlady donge juldo bust ave.\n",
      "I achaloint tireal ps thiktsthot c ik c.\n",
      "S ent r blys br chic.\n",
      "If mistod pal Extanlivednswoneakenthe ory as ong.\n",
      "Trs ald to ve wot.\n",
      "Yo t at it isundete tintes.\n",
      "Pil wn acase yof an ifares we st banger erenteve ptathan here se e s wiop cthofoutysthent bolir g.\n",
      "Anesoraren cortsushind ayo LAMyonexinodse ar it ckethin blinaf an med Cancouce atio owhe ikinor itharous I mmerird ren apeells.\n",
      "BGape I It an areh jut t isken m we ithary n t ourone tt.\n"
     ]
    }
   ],
   "source": [
    "for iterations in range(10):\n",
    "    index = 0\n",
    "    output = []\n",
    "    while True:\n",
    "        prob = a[index].float()\n",
    "        prob = prob / prob.sum()\n",
    "        index = index = torch.multinomial(prob , num_samples = 1 , replacement = True , generator = gen).item()\n",
    "        output.append(itos[index])\n",
    "        if index == 0:\n",
    "            break\n",
    "    print(''.join(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a loss function\n",
    "\"\"\"The idea is that we will take average negative log likelihood as our loss function.\n",
    "To find the loss function. \n",
    "1. we will find the probability matrix for lookup table(lookup_table/sum(lookup_table)). \n",
    "2. We then apply log function to priticular value sequence and add the values, Since log(a*b*c) = log(a)+log(b)+log(c). \n",
    "    For more details on this refer to cross-entropy - https://en.wikipedia.org/wiki/Cross_entropy \n",
    "    . since log values are proportional ot given values our goal is to maximise the log values.\n",
    "3. We will then apply negaitive sign to log value. we need to minimize this value for lower loss.\n",
    "4. We will then take average value of negative log value as our loss value.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Here we will also be adding some numeric value to all the lookup matrix values in order to smooth out the probabilities.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilites = a.float()\n",
    "probabilites /= probabilites.sum(1 , keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final loss: 2.394456624984741\n"
     ]
    }
   ],
   "source": [
    "#loss function\n",
    "log_likelihood = 0.0\n",
    "count = 0\n",
    "for i in lines:\n",
    "    chs = ['.'] + list(i) + ['.']\n",
    "    for ch1, ch2 in zip(chs , chs[1:]):\n",
    "        idx1 = stoi[ch1]\n",
    "        idx2 = stoi[ch2]\n",
    "        probability = probabilites[idx1 , idx2]\n",
    "        log_likelihood += torch.log(probability)\n",
    "        count += 1\n",
    "        #print(f'{ch1}{ch2}: {probability:.4f} {torch.log(probability):.4f}')\n",
    "#print(f'Loglikeihood: {log_likelihood}')\n",
    "neg_log_like = - log_likelihood\n",
    "#print(f'Negative Log Likelihood: {neg_log_like}')\n",
    "print(f'final loss: {neg_log_like / count :.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"idea is that instead of using lookup table we will use neural networks to predict what comes next. \n",
    "This is a simple network with only layer. Then we will use softmax to normalize the output layer and predict.\n",
    "The loss function is the same that is average negative log likeliehood. We will be passing a backward pass to update weights \n",
    "for better loss. The first layer is input layer that is indexes of ch1 and the output would be the indexes of ch2.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first step is to make the lookup matrix into input and output matrixes. \n",
    "xs , ys = [] , []\n",
    "for line in lines:\n",
    "    line = ['.'] + list(line) + ['.']\n",
    "    for ch1 , ch2 in zip(line , line[1:]):\n",
    "        idx1 = stoi[ch1]\n",
    "        idx2 = stoi[ch2]\n",
    "        xs.append(idx1)\n",
    "        ys.append(idx2)\n",
    "ys = torch.tensor(ys)\n",
    "xs = torch.tensor(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since these are integers we will be doing one-hot encoding on these\n",
    "#there is function for one-hot encoding in torch we will be utilizing it.\n",
    "import torch.nn.functional as f\n",
    "xonehot = f.one_hot(xs , num_classes = 54).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5555745"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xonehot.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will be intialising weights using torch.randn\n",
    "weights = torch.randn((54,54) , generator = gen , requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets have xs.w (dot product) in ys = xs.w + b\n",
    "#here since the shape of xs is 201 X 54 and shape of weights is 54 X 54, the dot product will have a shape of 201 X 54\n",
    "#forward pass\n",
    "\"\"\"The reason why I have used torch.arrange is it is similar to passing [0,1,2,3,...,201]. With this we will find the loss function\n",
    "just like we did in the lookup matrix approach\"\"\"\n",
    "logits = xonehot @ weights #log-counts or logits\n",
    "counts = logits.exp() #this tensor is equivalent to look up matrix\n",
    "probs = counts / counts.sum(1 , keepdims = True)\n",
    "loss = -probs[torch.arange(xonehot.shape[0]) , ys].log().mean() #this is our average negative log likelihood loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.493727684020996\n"
     ]
    }
   ],
   "source": [
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backward pass and updating the weights\n",
    "#this is a inbuilt function for torch lib. This calculates the gradients for weights wrt loss. \n",
    "#using these grad values we will be updating the weights in the direction of grads.\n",
    "weights.grad = None #this is equivalent to setting grads to zero except it saves space\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.data += -0.1 * weights.grad #updating weights with learning rate of 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----final loss:  2.5037\n"
     ]
    }
   ],
   "source": [
    "#training the network\n",
    "for i in range(100):\n",
    "    #forward pass\n",
    "    logits = xonehot @ weightse\n",
    "    counts = logits.exp()\n",
    "    #This is softmax layer\n",
    "    probs = counts / counts.sum(1 , keepdims = True)\n",
    "    loss = -probs[torch.arange(5555745) , ys].log().mean() + 0.01*(weights**2).mean() #added regularization\n",
    "    #print(f'-------iteration: {i}-----loss: {loss}')\n",
    "    #backward pass\n",
    "    weights.grad = None\n",
    "    loss.backward()\n",
    "    weights.data += -50 * weights.grad\n",
    "print(f'-----final loss: {loss: .4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We ucle ty cane rs wn wad acis crg o co ll arkersover a pen n lyo into y ng aVBxt.\n",
      "TheaWNoyoo tepBVGnippALEI fGEsuqzus.\n",
      " p achchice wadoplet d andoves s I d mybe omese enan t bl n tove g clele ctsAn t and foFQAjHvRihes beanRAlu time upen ou owithe.\n",
      "I ire I y thecutiventhere heoullRwouWXFyofest cif j.\n",
      "I mheaPgEqO y yorousAgCHhatU.\n",
      "Wjuorilinea akno knaroun aveas.\n",
      "It l fcOy lpalWwey hthanoraliof s f ingoudol.\n",
      "I ithe borembu ha ORODs te tD t thacarese.\n",
      "zue pYt ompheo s y inGFk thine rzXjHolThet l D lle je rqsthvere nthiuredun thine asn tsendrhad youl do eThecxf ght ilathoonde jl r lowkeefenloussulienshas icoveroraplem t ly t t sotoen.\n",
      "i o ourth vHkeay blyor.\n"
     ]
    }
   ],
   "source": [
    "#generating output from neural networks\n",
    "#idea is that using the weights from the trained model we will be forward passing the input and will using multinomial to \n",
    "#generate output. So, in a way the generator function is same as before.\n",
    "for iterations in range(10):\n",
    "    index = 0\n",
    "    output = []\n",
    "    while True:\n",
    "        #forward pass\n",
    "        xonehot_out = f.one_hot(torch.tensor([index]) , num_classes = 54).float()\n",
    "        logits_out = xonehot_out @ weights\n",
    "        counts_out = logits_out.exp()\n",
    "        #This is softmax layer\n",
    "        probs_out = counts_out / counts_out.sum(1 , keepdims = True)\n",
    "        index = torch.multinomial(probs_out , num_samples = 1 , replacement = True , generator = gen).item()\n",
    "        output.append(itos[index])\n",
    "        if index == 0:\n",
    "            break\n",
    "    print(''.join(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
